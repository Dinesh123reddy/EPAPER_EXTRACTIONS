{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dinesh123reddy/EPAPER_EXTRACTIONS/blob/main/VAARTHA_NEWS_PAPER_EXTRACTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN CODE**"
      ],
      "metadata": {
        "id": "rBbZMWlCYx2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Extracting Karimnagar Main Page URL**"
      ],
      "metadata": {
        "id": "hUTXM0W20409"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://epaper.vaartha.com'\n",
        "matching_files1 = []\n",
        "\n",
        "\n",
        "for date in range(1,32):\n",
        "      if len(str(date)) < 2:\n",
        "        date = f'0{date}'\n",
        "\n",
        "      primary_dir = f'/archive/date/{date}-Jan-2025?forcesingle=yes'\n",
        "      response = requests.get(url+primary_dir)\n",
        "\n",
        "      if response.status_code == 200:\n",
        "\n",
        "          soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "          links = soup.find_all('a')\n",
        "\n",
        "          for link in links:\n",
        "              file_name = link.get('href', '')\n",
        "              if \"/category/26/karimnagar-main\" in file_name:\n",
        "                  matching_files1.append(url+file_name)\n",
        "      else:\n",
        "        print(f\"Failed to retrieve the webpage. {url+primary_dir}\")\n",
        "\n",
        "matching_files1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtKzD6Q90rtZ",
        "outputId": "20cc74d5-f4c1-4a59-d045-ccab6dcd7cca",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://epaper.vaartha.com/category/26/karimnagar-main',\n",
              " 'https://epaper.vaartha.com/category/26/karimnagar-main',\n",
              " 'https://epaper.vaartha.com/category/26/karimnagar-main']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **EXTRACTING THE ALL KARIMNAGAR SUB PAGE URLS FROM THE MAIN URL**"
      ],
      "metadata": {
        "id": "A0ueAomyNpIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Folder to save downloaded PDF links (optional)\n",
        "download_folder = \"VAARTHA\"\n",
        "New_Urls = []\n",
        "\n",
        "if not os.path.exists(download_folder):\n",
        "    os.makedirs(download_folder)\n",
        "\n",
        "def extract_pdf_links_from_page(base_url):\n",
        "    \"\"\"\n",
        "    Extract all links matching the pattern from a single page.\n",
        "    \"\"\"\n",
        "    matching_files = []  # List to store extracted URLs\n",
        "    try:\n",
        "        # Fetch the webpage content\n",
        "        response = requests.get(base_url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all <a> tags with href containing '/vaartha'\n",
        "            links = soup.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                href = link['href']\n",
        "                if '/vaartha' in href:  # Filter links containing '/vaartha'\n",
        "                    full_url = urljoin(base_url, href)  # Construct full URL\n",
        "                    matching_files.append(full_url)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve {base_url}. HTTP Status Code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while extracting links from {base_url}: {e}\")\n",
        "    return matching_files\n",
        "\n",
        "def scrape_all_pages(base_url):\n",
        "    \"\"\"\n",
        "    Scrape only the first 3 pages of the website.\n",
        "    \"\"\"\n",
        "    all_links = []\n",
        "\n",
        "    for page_number in range(1,len(matching_files1)+1):  # Loop only through pages 1, 2, and 3\n",
        "        # Construct the page URL\n",
        "        current_page_url = f\"{base_url}/{page_number}\"\n",
        "        print(f\"Scraping: {current_page_url}\")\n",
        "        links = extract_pdf_links_from_page(current_page_url)\n",
        "\n",
        "        if links:\n",
        "            all_links.extend(links)  # Add extracted links to the master list\n",
        "        else:\n",
        "            print(f\"No links found on page {page_number}. Skipping.\")\n",
        "\n",
        "    return all_links\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL for the webpage\n",
        "    base_url = matching_files1[0]\n",
        "\n",
        "    # Scrape pages 1 to 3 for links\n",
        "    all_pdf_links = scrape_all_pages(base_url)\n",
        "\n",
        "    # Print unique links only\n",
        "    unique_links = list(set(all_pdf_links))\n",
        "    print(\"Extracted PDF URLs:\")\n",
        "    for url in unique_links:\n",
        "        print(url)\n",
        "\n",
        "    # Save unique links to a file (optional)\n",
        "    with open(os.path.join(download_folder, \"extracted_links\"), \"w\") as file:\n",
        "        for url in unique_links:\n",
        "            file.write(url + \"\\n\")\n",
        "\n",
        "    print(\"Script execution completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy2JM3I5Ja3J",
        "outputId": "44f4756a-b230-4d0f-e1ca-b871bf02aa34",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://epaper.vaartha.com/category/26/karimnagar-main/1\n",
            "Scraping: https://epaper.vaartha.com/category/26/karimnagar-main/2\n",
            "Scraping: https://epaper.vaartha.com/category/26/karimnagar-main/3\n",
            "Extracted PDF URLs:\n",
            "https://epaper.vaartha.com/view/3016/vaartha\n",
            "https://epaper.vaartha.com/view/3593/vaartha\n",
            "https://epaper.vaartha.com/view/2947/vaartha\n",
            "https://epaper.vaartha.com/view/3194/vaartha\n",
            "https://epaper.vaartha.com/view/1780/vaartha\n",
            "https://epaper.vaartha.com/view/3702/vaartha\n",
            "https://epaper.vaartha.com/view/3267/vaartha\n",
            "https://epaper.vaartha.com/category/26/karimnagar-main/instagram.com/vaarthadigital\n",
            "https://epaper.vaartha.com/view/3090/vaartha\n",
            "https://epaper.vaartha.com/view/3450/vaartha\n",
            "https://epaper.vaartha.com/view/3358/vaartha\n",
            "https://epaper.vaartha.com/view/3376/vaartha\n",
            "https://epaper.vaartha.com/view/3146/vaartha\n",
            "https://epaper.vaartha.com/view/3668/vaartha\n",
            "https://epaper.vaartha.com/view/3487/vaartha\n",
            "https://epaper.vaartha.com/view/3233/vaartha\n",
            "https://epaper.vaartha.com/view/2872/vaartha\n",
            "https://epaper.vaartha.com/view/3523/vaartha\n",
            "https://epaper.vaartha.com/view/2777/vaartha\n",
            "https://epaper.vaartha.com/view/3414/vaartha\n",
            "https://epaper.vaartha.com/view/3303/vaartha\n",
            "https://epaper.vaartha.com/view/2908/vaartha\n",
            "https://vaartha.com\n",
            "https://epaper.vaartha.com/view/3054/vaartha\n",
            "https://epaper.vaartha.com/view/2976/vaartha\n",
            "https://epaper.vaartha.com/view/3560/vaartha\n",
            "https://epaper.vaartha.com/view/3630/vaartha\n",
            "https://epaper.vaartha.com/view/3176/vaartha\n",
            "Script execution completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Downloading PDFS from the URLS**"
      ],
      "metadata": {
        "id": "-OCXKDccxGeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list of webpage URLs in a single variable\n",
        "WEBPAGE_URLS = [\n",
        "    'https://epaper.vaartha.com/view/3016/vaartha',\n",
        "'https://epaper.vaartha.com/view/3593/vaartha',\n",
        "'https://epaper.vaartha.com/view/2947/vaartha',\n",
        "'https://epaper.vaartha.com/view/3194/vaartha',\n",
        "'https://epaper.vaartha.com/view/1780/vaartha',\n",
        "'https://epaper.vaartha.com/view/3702/vaartha',\n",
        "'https://epaper.vaartha.com/view/3267/vaartha',\n",
        "'https://epaper.vaartha.com/category/26/karimnagar-main/instagram.com/vaarthadigital',\n",
        "'https://epaper.vaartha.com/view/3090/vaartha',\n",
        "'https://epaper.vaartha.com/view/3450/vaartha',\n",
        "'https://epaper.vaartha.com/view/3358/vaartha',\n",
        "'https://epaper.vaartha.com/view/3376/vaartha',\n",
        "'https://epaper.vaartha.com/view/3146/vaartha',\n",
        "'https://epaper.vaartha.com/view/3668/vaartha',\n",
        "'https://epaper.vaartha.com/view/3487/vaartha',\n",
        "'https://epaper.vaartha.com/view/3233/vaartha',\n",
        "'https://epaper.vaartha.com/view/2872/vaartha',\n",
        "'https://epaper.vaartha.com/view/3523/vaartha',\n",
        "'https://epaper.vaartha.com/view/2777/vaartha',\n",
        "'https://epaper.vaartha.com/view/3414/vaartha',\n",
        "'https://epaper.vaartha.com/view/3303/vaartha',\n",
        "'https://epaper.vaartha.com/view/2908/vaartha',\n",
        "'https://vaartha.com',\n",
        "'https://epaper.vaartha.com/view/3054/vaartha',\n",
        "'https://epaper.vaartha.com/view/2976/vaartha',\n",
        "'https://epaper.vaartha.com/view/3560/vaartha',\n",
        "'https://epaper.vaartha.com/view/3630/vaartha',\n",
        "'https://epaper.vaartha.com/view/3176/vaartha'\n",
        "]\n",
        "\n",
        "# Folder to save downloaded PDFs\n",
        "DOWNLOAD_FOLDER = \"VAARTHA\"\n",
        "if not os.path.exists(DOWNLOAD_FOLDER):\n",
        "    os.makedirs(DOWNLOAD_FOLDER)\n",
        "\n",
        "# Set to track already processed PDF links\n",
        "processed_links = set()\n",
        "\n",
        "# Function to get the date from the PDF headers\n",
        "def get_date_from_headers(response):\n",
        "    try:\n",
        "        last_modified = response.headers.get('Last-Modified')\n",
        "        if last_modified:\n",
        "            return datetime.strptime(last_modified, \"%a, %d %b %Y %H:%M:%S GMT\").strftime('%Y-%m-%d')\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing Last-Modified header: {e}\")\n",
        "    return \"unknown-date\"\n",
        "\n",
        "# Function to download PDFs from a single webpage\n",
        "def download_pdfs_from_webpage(webpage_url):\n",
        "    try:\n",
        "        # Step 1: Fetch the webpage\n",
        "        response = requests.get(webpage_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch {webpage_url}. HTTP Status Code: {response.status_code}\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Parse the HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 3: Find all PDF links\n",
        "        pdf_links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            if link['href'].endswith('.pdf'):\n",
        "                pdf_link = link['href']\n",
        "                if not pdf_link.startswith(\"http\"):\n",
        "                    pdf_link = requests.compat.urljoin(webpage_url, pdf_link)\n",
        "                pdf_links.append(pdf_link)\n",
        "\n",
        "        # Step 4: Download each unique PDF\n",
        "        if pdf_links:\n",
        "            print(f\"Found {len(pdf_links)} PDF links on {webpage_url}.\")\n",
        "            for pdf_link in pdf_links:\n",
        "                if pdf_link in processed_links:\n",
        "                    print(f\"Skipping already downloaded: {pdf_link}\")\n",
        "                    continue  # Skip already processed links\n",
        "\n",
        "                try:\n",
        "                    pdf_response = requests.get(pdf_link, stream=True)\n",
        "                    if pdf_response.status_code == 200:\n",
        "                        # Mark the link as processed\n",
        "                        processed_links.add(pdf_link)\n",
        "\n",
        "                        # Get the PDF date from headers\n",
        "                        pdf_date = get_date_from_headers(pdf_response)\n",
        "\n",
        "                        # Generate the filename\n",
        "                        filename = os.path.join(DOWNLOAD_FOLDER, f\"{pdf_date}.pdf\")\n",
        "\n",
        "                        # Save the PDF\n",
        "                        with open(filename, \"wb\") as file:\n",
        "                            for chunk in pdf_response.iter_content(chunk_size=1024):\n",
        "                                file.write(chunk)\n",
        "                        print(f\"Downloaded: {filename}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to download {pdf_link}. HTTP Status Code: {pdf_response.status_code}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading {pdf_link}: {e}\")\n",
        "        else:\n",
        "            print(f\"No PDF links found on {webpage_url}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {webpage_url}: {e}\")\n",
        "\n",
        "# Process each webpage\n",
        "for webpage_url in WEBPAGE_URLS:\n",
        "    download_pdfs_from_webpage(webpage_url)\n",
        "\n",
        "print(\"Script execution completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW0kZ6knztIb",
        "outputId": "f6f72a5b-6d0a-4889-edda-cd283b0baebb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 PDF links on https://epaper.vaartha.com/view/3016/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-01.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main1.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3593/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-18.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main15.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/2947/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-30.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2024-12/krm-main-(1).pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3194/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-06.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main6.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/1780/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-02.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2024-12/krm-main2.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3702/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-21.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main-(1)4.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3267/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-08.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main8.pdf\n",
            "No PDF links found on https://epaper.vaartha.com/category/26/karimnagar-main/instagram.com/vaarthadigital.\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3090/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-03.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main3.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3450/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-13.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main-(1).pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3358/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-10.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main10.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3376/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-11.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main11.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3146/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-04.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main4.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3668/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-20.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main-(1)3.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3487/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-15.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main13.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3233/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-07.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main7.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/2872/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-28.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2024-12/krm-main3.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3523/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-16.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main14.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/2777/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-27.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2024-12/krm-main.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3414/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-12.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main12.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3303/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-09.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main9.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/2908/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-29.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2024-12/krm-main4.pdf\n",
            "No PDF links found on https://vaartha.com.\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3054/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-02.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main2.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/2976/vaartha.\n",
            "Downloaded: VAARTHA/2024-12-31.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3560/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-17.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main-(1)1.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3630/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-19.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main-(1)2.pdf\n",
            "Found 2 PDF links on https://epaper.vaartha.com/view/3176/vaartha.\n",
            "Downloaded: VAARTHA/2025-01-05.pdf\n",
            "Skipping already downloaded: https://epaper.vaartha.com/media/2025-01/krm-main5.pdf\n",
            "Script execution completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Moving Files to the Zip Folder**"
      ],
      "metadata": {
        "id": "4U3saMNhw7f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the folder in Colab\n",
        "folder_path = '/content/VAARTHA'  # Replace with your folder path\n",
        "\n",
        "# Name of the ZIP file to create\n",
        "zip_file_name = 'VAARTHA-NEWS-PAPER.zip'\n",
        "\n",
        "# Compress the folder into a ZIP file\n",
        "shutil.make_archive(base_name='VAARTHA-NEWS-PAPER', format='zip', root_dir=folder_path)\n",
        "\n",
        "# Download the ZIP file to your local machine\n",
        "files.download(zip_file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-fubhhawu_bw",
        "outputId": "9366da51-f218-4a06-c7c2-5dacf99d34e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_614745cc-4173-4c30-8f3e-dffe101614d8\", \"VAARTHA-NEWS-PAPER.zip\", 92204384)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN CODE**"
      ],
      "metadata": {
        "id": "dkaaQt_u6iZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MOUNTING GOOGLE DRIVE**"
      ],
      "metadata": {
        "id": "jegdfW4XhPth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your drive\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# List folders\n",
        "folders = [f for f in os.listdir(drive_path) if os.path.isdir(os.path.join(drive_path, f))]"
      ],
      "metadata": {
        "id": "pBgMEMU7f5kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **DOWNLOADING PACKAGES AND IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "idOD1Spjhctx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils -y # Install system package for pdf2image\n",
        "!pip install pdf2image # Install the pdf2image python module\n",
        "!pip install --upgrade pdf2image # Upgrade pdf2image to the latest version\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from pdf2image.exceptions import PDFPageCountError # Import PDFPageCountError from exceptions submodule"
      ],
      "metadata": {
        "id": "8bDHEz47f5gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "import pytesseract\n",
        "import os"
      ],
      "metadata": {
        "id": "MBtJ2CWzf5eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "JuFQ5AI-f5cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y tesseract-ocr-tel # Install Telugu language data"
      ],
      "metadata": {
        "id": "rz-IgUTYf5Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "Zeipc1lLf5Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Got the URLS of the Each Date**"
      ],
      "metadata": {
        "id": "v9SHtSaH6r3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Folder to save downloaded PDF links (optional)\n",
        "download_folder = \"/content/drive/MyDrive/VAARTHA\"\n",
        "if not os.path.exists(download_folder):\n",
        "    os.makedirs(download_folder)\n",
        "\n",
        "if not os.path.exists(download_folder):\n",
        "    os.makedirs(download_folder)\n",
        "\n",
        "def extract_pdf_links_from_page(base_url):\n",
        "    \"\"\"\n",
        "    Extract all links matching the pattern from a single page.\n",
        "    \"\"\"\n",
        "    matching_files = set()  # Use a set to store unique URLs\n",
        "    try:\n",
        "        # Fetch the webpage content\n",
        "        response = requests.get(base_url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all <a> tags with href containing '/vaartha'\n",
        "            links = soup.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                href = link['href']\n",
        "                for date in range(1, 32):\n",
        "                    if f'{date} Jan 2025' in href:  # Filter links containing '/vaartha'\n",
        "                        full_url = urljoin(base_url, href)  # Construct full URL\n",
        "                        matching_files.add(full_url)  # Add to set (avoids duplicates)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve {base_url}. HTTP Status Code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while extracting links from {base_url}: {e}\")\n",
        "\n",
        "    return matching_files  # Return unique links\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL for the webpage\n",
        "    base_url = 'https://archive-epapervaartha.com/index.php?category=Karimnagar%20Tab&month=2025-01'\n",
        "    matching_files = extract_pdf_links_from_page(base_url)\n",
        "\n",
        "    # Print the unique URLs\n",
        "    for url in matching_files:\n",
        "        print(url)\n",
        "\n",
        "    # Save unique links to a file (optional)\n",
        "    with open(os.path.join(download_folder, \"extracted_links.txt\"), \"w\") as file:\n",
        "        for url in matching_files:\n",
        "            file.write(url + \"\\n\")\n",
        "\n",
        "    print(\"Script execution completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOLEdfGtFY-O",
        "outputId": "02447068-4609-46ee-b6ef-13ea31c8694c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3488-16 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3304-10 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3091-04 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3415-13 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3339-11 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3561-18 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3177-06 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3268-09 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3234-08 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3377-12 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3451-14 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3522-17 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3126-05 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3017-02 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3055-03 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3195-07 Jan 2025\n",
            "https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=2977-01 Jan 2025\n",
            "Script execution completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Downloaded all the PDF's from the Generated URLS**"
      ],
      "metadata": {
        "id": "Fs_cM8UpJc5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list of webpage URLs in a single variable\n",
        "WEBPAGE_URLS = ['https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=2977-01 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3017-02 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3055-03 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3091-04 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3126-05 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3177-06 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3195-07 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3234-08 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3268-09 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3304-10 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3339-11 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3339-11 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3377-12 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3377-12 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3415-13 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3415-13 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3451-14 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3451-14 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3488-16 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3488-16 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3522-17 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3522-17 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3561-18 Jan 2025',\n",
        "'https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3561-18 Jan 2025']\n",
        "\n",
        "# Folder to save downloaded PDFs\n",
        "DOWNLOAD_FOLDER = \"VAARTHA-NEWS-PAPER-Jan\"\n",
        "if not os.path.exists(DOWNLOAD_FOLDER):\n",
        "    os.makedirs(DOWNLOAD_FOLDER)\n",
        "\n",
        "# Set to track already processed PDF links\n",
        "processed_links = set()\n",
        "\n",
        "# Function to extract the date of the newspaper from the webpage\n",
        "def extract_newspaper_date(webpage_url, soup):\n",
        "    try:\n",
        "        # Check the webpage title or specific elements containing the date\n",
        "        title = soup.title.string if soup.title else \"\"\n",
        "        if \"Jan 2025\" in title:\n",
        "            # Extract date from the title\n",
        "            for word in title.split():\n",
        "                if word.isdigit():  # Check for a numeric day\n",
        "                    day = int(word)\n",
        "                    return f\"2025-01-{day:02}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting date from {webpage_url}: {e}\")\n",
        "    return \"unknown-date\"\n",
        "\n",
        "# Function to download PDFs from a single webpage\n",
        "def download_pdfs_from_webpage(webpage_url):\n",
        "    try:\n",
        "        # Step 1: Fetch the webpage\n",
        "        response = requests.get(webpage_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch {webpage_url}. HTTP Status Code: {response.status_code}\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Parse the HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 3: Extract newspaper date from the webpage\n",
        "        newspaper_date = extract_newspaper_date(webpage_url, soup)\n",
        "\n",
        "        # Step 4: Find all PDF links\n",
        "        pdf_links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            if link['href'].endswith('.pdf'):\n",
        "                pdf_link = link['href']\n",
        "                if not pdf_link.startswith(\"http\"):\n",
        "                    pdf_link = requests.compat.urljoin(webpage_url, pdf_link)\n",
        "                pdf_links.append(pdf_link)\n",
        "\n",
        "        # Step 5: Download each unique PDF\n",
        "        if pdf_links:\n",
        "            print(f\"Found {len(pdf_links)} PDF links on {webpage_url}.\")\n",
        "            for pdf_link in pdf_links:\n",
        "                if pdf_link in processed_links:\n",
        "                    print(f\"Skipping already downloaded: {pdf_link}\")\n",
        "                    continue  # Skip already processed links\n",
        "\n",
        "                try:\n",
        "                    pdf_response = requests.get(pdf_link, stream=True)\n",
        "                    if pdf_response.status_code == 200:\n",
        "                        # Mark the link as processed\n",
        "                        processed_links.add(pdf_link)\n",
        "\n",
        "                        # Generate the filename\n",
        "                        filename = os.path.join(DOWNLOAD_FOLDER, f\"{newspaper_date}.pdf\")\n",
        "\n",
        "                        # Save the PDF\n",
        "                        with open(filename, \"wb\") as file:\n",
        "                            for chunk in pdf_response.iter_content(chunk_size=1024):\n",
        "                                file.write(chunk)\n",
        "                        print(f\"Downloaded: {filename}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to download {pdf_link}. HTTP Status Code: {pdf_response.status_code}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading {pdf_link}: {e}\")\n",
        "        else:\n",
        "            print(f\"No PDF links found on {webpage_url}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {webpage_url}: {e}\")\n",
        "\n",
        "# Process each webpage\n",
        "for webpage_url in WEBPAGE_URLS:\n",
        "    download_pdfs_from_webpage(webpage_url)\n",
        "\n",
        "print(\"Script execution completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEYInTSfJbyq",
        "outputId": "62578b11-fb54-4714-bf72-45d99e2f7095",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=2977-01 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-01.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3017-02 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-02.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3055-03 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-03.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3091-04 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-04.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3126-05 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-05.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3177-06 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-06.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3195-07 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-07.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3234-08 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-08.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3268-09 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-09.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3304-10 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-10.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3339-11 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-11.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3339-11 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3339-11 Jan 2025/krm-tab-(1).pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3377-12 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-12.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3377-12 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3377-12 Jan 2025/knr-tab-(1).pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3415-13 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-13.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3415-13 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3415-13 Jan 2025/krm-tab10.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3451-14 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-14.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3451-14 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3451-14 Jan 2025/krm-tab-(1)1.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3488-16 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-16.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3488-16 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3488-16 Jan 2025/krm-tab11.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3522-17 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-17.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3522-17 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3522-17 Jan 2025/krm-tab12.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3561-18 Jan 2025.\n",
            "Downloaded: VAARTHA-NEWS-PAPER-Jan/2025-01-18.pdf\n",
            "Found 1 PDF links on https://archive-epapervaartha.com/epaper.php?category=Karimnagar Tab&month=2025-01&epaper=3561-18 Jan 2025.\n",
            "Skipping already downloaded: http://archive-epapervaartha.com/media/Karimnagar Tab/2025-01/3561-18 Jan 2025/krm-tab-(1)2.pdf\n",
            "Script execution completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **CONVERTING PDF TO .JPG IMAGES**"
      ],
      "metadata": {
        "id": "h8pKiAq1iouB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "import fitz  # PyMuPDF for PDF validation\n",
        "from pdf2image import convert_from_path\n",
        "import re  # Regular expressions for date extraction\n",
        "\n",
        "# Mapping month numbers and lowercase month names to title-case format\n",
        "MONTHS_MAP = {\n",
        "    \"01\": \"JAN\", \"02\": \"FEB\", \"03\": \"MAR\", \"04\": \"APR\",\n",
        "    \"05\": \"MAY\", \"06\": \"JUN\", \"07\": \"JUL\", \"08\": \"AUG\",\n",
        "    \"09\": \"SEP\", \"10\": \"OCT\", \"11\": \"NOV\", \"12\": \"DEC\",\n",
        "    \"jan\": \"JAN\", \"feb\": \"FEB\", \"mar\": \"MAR\", \"apr\": \"APR\",\n",
        "    \"may\": \"MAY\", \"jun\": \"JUN\", \"jul\": \"JUL\", \"aug\": \"AUG\",\n",
        "    \"sep\": \"SEP\", \"oct\": \"OCT\", \"nov\": \"NOV\", \"dec\": \"DEC\"\n",
        "}\n",
        "\n",
        "# Input and output folders\n",
        "input_folder = \"/content/drive/MyDrive/VAARTHA DATA\"\n",
        "output_folder = \"/content/drive/MyDrive/VAARTHA\"\n",
        "os.makedirs(output_folder, exist_ok=True)  # Ensure output directory exists\n",
        "\n",
        "# ✅ Function to extract date from filename\n",
        "def extract_date_from_filename(filename):\n",
        "    filename = filename.lower()  # Convert to lowercase for better matching\n",
        "\n",
        "    # Format 1: \"JAN 12, 2025.pdf\" (Handles lowercase & missing spaces)\n",
        "    match1 = re.search(r'([a-zA-Z]{3,})\\s*(\\d{1,2}),?\\s*(\\d{4})', filename)\n",
        "\n",
        "    if match1:\n",
        "        month_text, day, year = match1.groups()\n",
        "        month_text = month_text[:3]  # Get first 3 letters\n",
        "        month = MONTHS_MAP.get(month_text, \"Unknown\")  # Convert to title-case format\n",
        "        if month == \"Unknown\":\n",
        "            return None\n",
        "        day = day.zfill(2)  # Ensure day is two digits (3 → 03)\n",
        "        return year, month, day\n",
        "\n",
        "    # Format 2: \"extracted_page_5_2024-10-24.pdf\"\n",
        "    match2 = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', filename)\n",
        "\n",
        "    if match2:\n",
        "        year, month_num, day = match2.groups()\n",
        "        month = MONTHS_MAP.get(month_num, \"Unknown\")  # Convert month number to name\n",
        "        if month == \"Unknown\":\n",
        "            return None\n",
        "        return year, month, day\n",
        "\n",
        "    return None\n",
        "\n",
        "# ✅ Function to process a single PDF\n",
        "def process_pdf(pdf_path):\n",
        "    file = os.path.basename(pdf_path)  # Get file name\n",
        "    pdf_name, _ = os.path.splitext(file)  # Remove .pdf extension\n",
        "\n",
        "    # ✅ Remove 'extracted_page_5_' prefix if it exists\n",
        "    pdf_name = re.sub(r'^extracted_page_\\d+_', '', pdf_name)\n",
        "\n",
        "    # Extract date from filename\n",
        "    extracted_date = extract_date_from_filename(pdf_name)\n",
        "    if not extracted_date:\n",
        "        print(f\"❌ Skipping '{file}' (Date format incorrect)\")\n",
        "        return\n",
        "\n",
        "    year, month, day = extracted_date\n",
        "\n",
        "    # ✅ Create Month Folder inside the output directory (e.g., \"Jan\", \"Dec\")\n",
        "    month_folder_path = os.path.join(output_folder, f\"VAARTHA-{month}-{year}\")\n",
        "    os.makedirs(month_folder_path, exist_ok=True)\n",
        "\n",
        "    # ✅ Create Date Folder inside the Month Folder (e.g., \"NAVA_TELANGANA_2025-Jan-02\")\n",
        "    date_folder = os.path.join(month_folder_path, f\"VAARTHA_{year}-{month}-{day}\")\n",
        "    os.makedirs(date_folder, exist_ok=True)\n",
        "\n",
        "    # ✅ Define output image file name (e.g., \"2025-Jan-02.jpg\")\n",
        "    output_filename = f\"{year}-{month}-{day}.jpg\"\n",
        "    output_path = os.path.join(date_folder, output_filename)\n",
        "\n",
        "    # ✅ Skip if the image already exists (Prevents duplication)\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"⏩ Skipping '{file}' (Already Processed)\")\n",
        "        return\n",
        "\n",
        "    # ✅ Validate the PDF\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            if doc.page_count == 0:\n",
        "                print(f\"❌ Skipping '{file}' (Empty or Invalid PDF)\")\n",
        "                return\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping '{file}' (Not a valid PDF): {e}\")\n",
        "        return\n",
        "\n",
        "    # ✅ Convert the first page of the PDF to an image\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing '{file}': {e}\")\n",
        "        return\n",
        "\n",
        "    # ✅ Save the first page as a JPG file\n",
        "    if images:\n",
        "        final_image = images[0]  # Use only the first page\n",
        "        final_image.save(output_path, \"JPEG\")\n",
        "        print(f\"✅ PDF '{file}' converted and saved as '{output_filename}' in '{date_folder}'.\")\n",
        "\n",
        "# ✅ Main processing loop with threading\n",
        "def process_all_pdfs():\n",
        "    month_folders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
        "\n",
        "    # ✅ Use ThreadPoolExecutor for parallel processing\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:  # Adjust max_workers if needed\n",
        "        futures = []\n",
        "\n",
        "        for month_folder in month_folders:\n",
        "            month_folder_path = os.path.join(input_folder, month_folder)\n",
        "\n",
        "            # Get all PDF files in the month folder\n",
        "            pdf_files = [\n",
        "                os.path.join(root, file)\n",
        "                for root, _, files in os.walk(month_folder_path)\n",
        "                for file in files if file.lower().endswith('.pdf')\n",
        "            ]\n",
        "\n",
        "            # ✅ Submit each PDF for processing in parallel\n",
        "            for pdf_file in pdf_files:\n",
        "                futures.append(executor.submit(process_pdf, pdf_file))\n",
        "\n",
        "        # ✅ Wait for all threads to complete\n",
        "        concurrent.futures.wait(futures)\n",
        "\n",
        "    print(\"🎉 All valid PDFs processed successfully!\")\n",
        "\n",
        "# ✅ Run the script\n",
        "process_all_pdfs()"
      ],
      "metadata": {
        "id": "cr3cepTYgSNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow==1.1.48 --quiet\n",
        "import roboflow"
      ],
      "metadata": {
        "id": "uMfacDZigTcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **CROPPING OF ARTICLES FROM THE .JPG IMAGE**"
      ],
      "metadata": {
        "id": "XMY9wRnBi1W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ Path to the folder containing images from the previous process\n",
        "output_folder = \"/content/drive/MyDrive/VAARTHA\"\n",
        "\n",
        "# ✅ Initialize Roboflow once\n",
        "rf = Roboflow(api_key=\"csNVQ6kkdgzTU6rKRr3V\")\n",
        "project = rf.workspace(\"koni-9lsug\").project(\"extraction-kzuf3\")\n",
        "model = project.version(5).model\n",
        "\n",
        "# ✅ Loop through all month folders in \"NAVA TELANGANA ARTICLES\"\n",
        "for month_folder in os.listdir(output_folder):\n",
        "    month_folder_path = os.path.join(output_folder, month_folder)\n",
        "\n",
        "    if not os.path.isdir(month_folder_path):\n",
        "        continue  # Skip if it's not a folder\n",
        "\n",
        "    # ✅ Loop through each date folder (e.g., \"NAVA_TELANGANA_2025-01-03\")\n",
        "    for date_folder in os.listdir(month_folder_path):\n",
        "        date_folder_path = os.path.join(month_folder_path, date_folder)\n",
        "\n",
        "        if not os.path.isdir(date_folder_path):\n",
        "            continue  # Skip if it's not a folder\n",
        "\n",
        "        # ✅ Use folder name directly for article naming (e.g., \"NAVA_TELANGANA_2025-01-03\")\n",
        "        folder_name = date_folder  # Full folder name (NAVA_TELANGANA_YYYY-MM-DD)\n",
        "\n",
        "        # ✅ Get the original image\n",
        "        image_files_in_date_folder = [\n",
        "            f for f in os.listdir(date_folder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
        "        ]\n",
        "\n",
        "        # ✅ Process each image in the date folder\n",
        "        for image_file in image_files_in_date_folder:\n",
        "            image_path = os.path.join(date_folder_path, image_file)\n",
        "\n",
        "            # ✅ Run model prediction\n",
        "            try:\n",
        "                predictions = model.predict(image_path, confidence=40, overlap=30).json()\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing '{image_file}': {e}\")\n",
        "                continue  # Skip if there's an issue with Roboflow API\n",
        "\n",
        "            # ✅ Load the original image\n",
        "            try:\n",
        "                original_image = Image.open(image_path)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error opening image '{image_file}': {e}\")\n",
        "                continue\n",
        "\n",
        "            # ✅ Extract and save article images\n",
        "            for i, prediction in enumerate(predictions.get('predictions', [])):\n",
        "                x, y, width, height = prediction[\"x\"], prediction[\"y\"], prediction[\"width\"], prediction[\"height\"]\n",
        "\n",
        "                # Convert center (x, y) to top-left (x1, y1) and bottom-right (x2, y2)\n",
        "                x1 = int(x - width / 2)\n",
        "                y1 = int(y - height / 2)\n",
        "                x2 = int(x + width / 2)\n",
        "                y2 = int(y + height / 2)\n",
        "\n",
        "                # ✅ Crop the detected region\n",
        "                cropped_image = original_image.crop((x1, y1, x2, y2))\n",
        "\n",
        "                # ✅ Save cropped article image with proper naming\n",
        "                article_filename = f\"{folder_name}_{i + 1}.jpg\"  # Example: \"NAVA_TELANGANA_2025-02-16_article_1.jpg\"\n",
        "                article_path = os.path.join(date_folder_path, article_filename)\n",
        "                cropped_image.save(article_path)\n",
        "\n",
        "            print(f\"✅ Articles extracted and saved in '{date_folder_path}' for '{image_file}'.\")\n",
        "\n",
        "print(\"🎉 Article extraction completed for all images!\")"
      ],
      "metadata": {
        "id": "JyGIKXUNgW2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **EXTRACTING AND TRANSLATING CONTENT FROM THE ARTICLES**"
      ],
      "metadata": {
        "id": "n4gq2vVEi70H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "from roboflow import Roboflow\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from googletrans import Translator\n",
        "\n",
        "# ✅ Path to the extracted articles folder\n",
        "output_folder = \"/content/drive/MyDrive/VAARTHA\"\n",
        "\n",
        "# ✅ Initialize Roboflow for article extraction\n",
        "rf = Roboflow(api_key=\"csNVQ6kkdgzTU6rKRr3V\")\n",
        "project = rf.workspace(\"koni-9lsug\").project(\"extraction-kzuf3\")\n",
        "model = project.version(5).model\n",
        "\n",
        "# ✅ Initialize Google Translate\n",
        "translator = Translator()\n",
        "\n",
        "# ✅ Configure Tesseract OCR for Telugu\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"  # Adjust for your system\n",
        "tesseract_lang = \"tel\"  # Telugu language code\n",
        "\n",
        "# ✅ Function to process a single article image\n",
        "def process_article(image_path, date_folder_path):\n",
        "    article_image = os.path.basename(image_path)\n",
        "\n",
        "    # ✅ Define output text file paths\n",
        "    text_filename_tel = os.path.splitext(article_image)[0] + \"_Telugu.txt\"\n",
        "    text_filename_eng = os.path.splitext(article_image)[0] + \"_English.txt\"\n",
        "    text_filepath_tel = os.path.join(date_folder_path, f\"VAARTHA_{text_filename_tel}\")\n",
        "    text_filepath_eng = os.path.join(date_folder_path, f\"VAARTHA_{text_filename_eng}\")\n",
        "\n",
        "    # ✅ Check if both Telugu and English text files already exist\n",
        "    if os.path.exists(text_filepath_tel) and os.path.exists(text_filepath_eng):\n",
        "        print(f\"⏩ Skipping '{article_image}' (Already Processed)\")\n",
        "        return  # Skip this file\n",
        "\n",
        "    # ✅ Extract Telugu text using Tesseract OCR\n",
        "    try:\n",
        "        telugu_text = pytesseract.image_to_string(Image.open(image_path), lang=tesseract_lang)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OCR failed for '{article_image}': {e}\")\n",
        "        return\n",
        "\n",
        "    # ✅ Translate Telugu text to English\n",
        "    try:\n",
        "        translated_text = translator.translate(telugu_text, src=\"te\", dest=\"en\").text\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Translation failed for '{article_image}': {e}\")\n",
        "        translated_text = \"Translation failed.\"\n",
        "\n",
        "    # ✅ Save extracted text and translation\n",
        "    with open(text_filepath_tel, \"w\", encoding=\"utf-8\") as text_file_tel, open(text_filepath_eng, \"w\", encoding=\"utf-8\") as text_file_eng:\n",
        "        text_file_tel.write(f\"📜 **Extracted Telugu Text:**\\n{telugu_text}\\n\\n\")\n",
        "        text_file_eng.write(f\"🌍 **Translated English Text:**\\n{translated_text}\\n\")\n",
        "\n",
        "    print(f\"✅ Text extracted and translated for '{article_image}' -> Saved as '{text_filename_eng}'.\")\n",
        "\n",
        "# ✅ Loop through month folders\n",
        "for month_folder in os.listdir(output_folder):\n",
        "    month_folder_path = os.path.join(output_folder, month_folder)\n",
        "\n",
        "    if not os.path.isdir(month_folder_path):\n",
        "        continue  # Skip if it's not a folder\n",
        "\n",
        "    # ✅ Loop through each date folder (e.g., \"NAVA_TELANGANA_2025-01-03\")\n",
        "    for date_folder in os.listdir(month_folder_path):\n",
        "        date_folder_path = os.path.join(month_folder_path, date_folder)\n",
        "\n",
        "        if not os.path.isdir(date_folder_path):\n",
        "            continue  # Skip if it's not a folder\n",
        "\n",
        "        # ✅ Get article images\n",
        "        article_images = [\n",
        "            os.path.join(date_folder_path, f)\n",
        "            for f in os.listdir(date_folder_path)\n",
        "            if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
        "        ]\n",
        "\n",
        "        # ✅ Process multiple images in parallel using ThreadPoolExecutor\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
        "            executor.map(process_article, article_images, [date_folder_path] * len(article_images))\n",
        "\n",
        "print(\"🎉 Telugu text extraction and translation completed for all articles!\")"
      ],
      "metadata": {
        "id": "DW1nZ99VgXDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Moving Files to the ZIP Folder**"
      ],
      "metadata": {
        "id": "iIR7xGFX7P4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the folder in Colab\n",
        "folder_path = '/content/VAARTHA-NEWS-PAPER-PDFS'  # Replace with your folder path\n",
        "\n",
        "# Name of the ZIP file to create\n",
        "zip_file_name = 'VAARTHA-NEWS-PAPER-PDFS.zip'\n",
        "\n",
        "# Compress the folder into a ZIP file\n",
        "shutil.make_archive(base_name='VAARTHA-NEWS-PAPER-PDFS', format='zip', root_dir=folder_path)\n",
        "\n",
        "# Download the ZIP file to your local machine\n",
        "files.download(zip_file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XqwXrVch7NnJ",
        "outputId": "6ac5d0f8-4ad5-4f02-a0e9-6f5bfc97285e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c520800c-0d24-4764-9579-973c822dd67a\", \"VAARTHA-NEWS-PAPER-PDFS.zip\", 405225076)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **For Unzipping Folders or Files**"
      ],
      "metadata": {
        "id": "zrxzs2LYR5VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = \"/content/VAARTHA-NEWS-PAPER-PDFS.zip\"\n",
        "\n",
        "# Destination folder to extract contents\n",
        "extract_to = \"C:\\canon\\Downloads-9164-39C4925E467B}\\Downloads\\NEWS PAPERS\"\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Contents extracted to {extract_to}\")\n"
      ],
      "metadata": {
        "id": "Ckm1Y0r7R4uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3dZj02lfnFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXOBfxBJfmwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_ePdnmkfmZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0k2GeQEhYMXONyjTgNQjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}